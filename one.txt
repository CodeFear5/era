Code:
import java.io.IOException; 
import java.util.*; 
import org.apache.hadoop.conf.Configuration; 
import org.apache.hadoop.fs.Path; 
import org.apache.hadoop.io.*; 
import org.apache.hadoop.mapreduce.*; 
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; 
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 
public class MatrixMultiply { 
public static class MatrixMapper extends Mapper<LongWritable, Text, Text, Text> 
{ 
private int p = 2; // columns in B 
private int m = 2; // rows in A 
public void map(LongWritable key, Text value, Context context) 
throws IOException, InterruptedException { 
String[] parts = value.toString().split(","); 
String matrix = parts[0]; 
int i = Integer.parseInt(parts[1]); 
            int j = Integer.parseInt(parts[2]); 
            double v = Double.parseDouble(parts[3]); 
 
            if (matrix.equals("A")) { 
                for (int col = 0; col < p; col++) { 
                    context.write(new Text(i + "," + col), new Text("A," + j + "," 
+ v)); 
                } 
            } else { 
                for (int row = 0; row < m; row++) { 
                    context.write(new Text(row + "," + j), new Text("B," + i + "," 
+ v)); 
                } 
            } 
        } 
    } 
 
    public static class MatrixReducer extends Reducer<Text, Text, Text, 
DoubleWritable> { 
        public void reduce(Text key, Iterable<Text> values, Context context) 
                throws IOException, InterruptedException { 
 
            Map<Integer, Double> A = new HashMap<>(); 
            Map<Integer, Double> B = new HashMap<>(); 
 
            for (Text val : values) { 
                String[] parts = val.toString().split(","); 
                if (parts[0].equals("A")) { 
                    A.put(Integer.parseInt(parts[1]), 
Double.parseDouble(parts[2])); 
                } else { 
                    B.put(Integer.parseInt(parts[1]), 
Double.parseDouble(parts[2])); 
                } 
            } 
 
            double sum = 0.0; 
            for (int k : A.keySet()) { 
                if (B.containsKey(k)) { 
                    sum += A.get(k) * B.get(k); 
                } 
            } 
 
            context.write(key, new DoubleWritable(sum)); 
        } 
    } 
 
    public static void main(String[] args) throws Exception { 
        Configuration conf = new Configuration(); 
        Job job = Job.getInstance(conf, "Matrix Multiplication"); 
 
        job.setJarByClass(MatrixMultiply.class); 
        job.setMapperClass(MatrixMapper.class); 
        job.setReducerClass(MatrixReducer.class); 
 
        job.setMapOutputKeyClass(Text.class); 
        job.setMapOutputValueClass(Text.class); 
job.setOutputKeyClass(Text.class); 
job.setOutputValueClass(DoubleWritable.class); 
FileInputFormat.addInputPath(job, new Path(args[0])); 
FileOutputFormat.addOutputPath(job, new Path(args[1])); 
System.exit(job.waitForCompletion(true) ? 0 : 1); 
} 
} 



4. Create Input Data 
Create a text file on your Cloudera desktop named matrix.txt: 
A,0,0,1 
A,0,1,2 
A,1,0,3 
A,1,1,4 
B,0,0,5 
B,0,1,6 
B,1,0,7 
B,1,1,8 

100% working commands

[cloudera@quickstart Desktop]$ hadoop dfs -mkdir /matin 
[cloudera@quickstart Desktop]$ hadoop dfs -copyFromLocal matrix.txt /matin 
[cloudera@quickstart Desktop]$ hadoop dfs -cat /user/root/matin/matrix.txt 
[cloudera@quickstart Desktop]$ hadoop jar MatMul.jar MatMul /user/root/matin 
/user/root/matout 
[cloudera@quickstart Desktop]$ hadoop dfs -cat /user/root/matout/part-r-00000 
